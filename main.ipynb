{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and define custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIRARIES\n",
    "#----------------------------------------------------------------\n",
    "# default libraries\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from time import time\n",
    "import glob\n",
    "import math\n",
    "from functools import cache, wraps\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "# machine learning libraries\n",
    "from keras.layers import Conv2D, Input, Dense, Dropout, MaxPool2D, UpSampling2D, Add\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import image_dataset_from_directory, split_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "# generic image functionality library\n",
    "from skimage import data, img_as_float\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.draw import disk\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# image manipulation library\n",
    "from scipy.fft import fft2, ifft2\n",
    "from scipy.signal import fftconvolve as fftc\n",
    "import cv2\n",
    "\n",
    "# image deconvolution and metric library\n",
    "from skimage.restoration import wiener, unsupervised_wiener, richardson_lucy\n",
    "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# suppress warnings\n",
    "\"\"\" import warnings\n",
    "warnings.filterwarnings('ignore') \"\"\"\n",
    "\n",
    "print(f'Libraries have been loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE CUSTOM FUNCTIONS\n",
    "#----------------------------------------------------------------\n",
    "# timing functionality\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        dur= te-ts\n",
    "        print(f'func: {f.__name__} took: {dur:.3} sec')\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "# apply timing to imported functions\n",
    "wiener_t = timing(wiener)\n",
    "unsupervised_wiener_t = timing(unsupervised_wiener)\n",
    "richardson_lucy_t = timing(richardson_lucy)\n",
    "\n",
    "# presentation functions\n",
    "def plot_images(images, titles=['undefined'], make_title=True, n_split=4, fname=[], scale=[0,1], colormap='gray'):\n",
    "    # source: https://github.com/TristanvanLeeuwen/Summerschool\n",
    "    \"\"\" helper function for plotting (originally K)\n",
    "    Note: quite significantly modified for personal use \"\"\"\n",
    "    n = np.minimum(n_split, len(images))\n",
    "    m = int(np.ceil(len(images)/n))\n",
    "    img_array = [images[i * n:(i + 1) * n] for i in range((len(images) + n - 1) // n )]\n",
    "    title_array = [titles[i * n:(i + 1) * n] for i in range((len(titles) + n - 1) // n )]\n",
    "    fig, ax = plt.subplots(m, n, squeeze=False, figsize=(n*4,m*4))\n",
    "\n",
    "    for row, k in enumerate(img_array):\n",
    "        for col, l in enumerate(k):\n",
    "            ax[row,col].set_xticks([])\n",
    "            ax[row,col].set_yticks([])\n",
    "            if scale == False:\n",
    "                ax[row,col].imshow(img_array[row][col], cmap=colormap)\n",
    "            else:\n",
    "                ax[row,col].imshow(img_array[row][col], cmap=colormap, vmin=scale[0], vmax=scale[1])\n",
    "            if make_title:\n",
    "                try:\n",
    "                    ax[row,col].set_title(title_array[row][col],fontsize=16)\n",
    "                except IndexError:\n",
    "                    ax[row,col].set_title(r'$undefined$',fontsize=16)\n",
    "            \n",
    "    list_c = [i for i in range(len(ax.flatten())) if i not in range(len(images))]\n",
    "    for i in list_c:\n",
    "        fig.delaxes(ax.flatten()[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if fname:\n",
    "        plt.savefig(fname,dpi=300)\n",
    "\n",
    "def single(image, size=None, osx=0, osy=0, colormap='gray', **kwargs):\n",
    "    \"\"\" function to show a single image, or part of an image, without labels or axes \"\"\"\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.tick_params(left=False,\n",
    "                    bottom=False,\n",
    "                    labelleft=False,\n",
    "                    labelbottom=False)\n",
    "    if size is not None:\n",
    "        plt.imshow(image[osx:osx+size,osy:osy+size], cmap=colormap, **kwargs)\n",
    "    else:\n",
    "        plt.imshow(image, cmap=colormap, **kwargs)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_quant(name, img1, img2, sig=3):\n",
    "    \"\"\" print MSE, PSNR adn SSIM for two input images, with 'sig' number of significant numbers \"\"\"\n",
    "    img1 = normalize_image(img1)\n",
    "    mse = mean_squared_error(img1,img2)\n",
    "    psnr = peak_signal_noise_ratio(img1,img2)\n",
    "    ssim = structural_similarity(img1,img2)\n",
    "    print(f'Metrics for {name} are MSE: {mse:.3}, PSNR: {psnr:.3}, SSIM: {ssim:.3}')\n",
    "\n",
    "def get_quant(img1, img2):\n",
    "    img1 = normalize_image(img1)\n",
    "    mse = mean_squared_error(img1,img2)\n",
    "    psnr = peak_signal_noise_ratio(img1,img2)\n",
    "    ssim = structural_similarity(img1,img2)\n",
    "    cm = (-1*mse)+(psnr/30)+ssim\n",
    "    return mse,psnr,ssim,cm\n",
    "\n",
    "def plot_quants(params, quants, title=None, xl='parameter', cl=True, interval=1, legloc=7, cm=False, optweak=False, offset=True):\n",
    "    # source: https://stackoverflow.com/a/45925049\n",
    "    #----------------------------------------------------------------\n",
    "    fig, host = plt.subplots(figsize=(8,5)) # (width, height) in inches\n",
    "    # (see https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.subplots.html)\n",
    "        \n",
    "    par1 = host.twinx()\n",
    "    par2 = host.twinx()\n",
    "    if cm:\n",
    "        par3 = host.twinx()\n",
    "\n",
    "    host.set_ylabel(\"MSE\")\n",
    "    par1.set_ylabel(\"PSNR\")\n",
    "    par2.set_ylabel(\"SSIM\")\n",
    "    if cm:\n",
    "        par3.set_ylabel(\"CM\")\n",
    "        par3.get_yaxis().set_visible(False)\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=12)\n",
    "    if optweak:\n",
    "        p1, = host.plot(params, quants[:,0], color='tab:green', label=\"MSE\", alpha=0.5)\n",
    "        p2, = par1.plot(params, quants[:,1], color='tab:blue', label=\"PSNR\", alpha=0.5)\n",
    "        p3, = par2.plot(params, quants[:,2], color='tab:orange', label=\"SSIM\", alpha=0.5)\n",
    "    else:\n",
    "        p1, = host.plot(params, quants[:,0], color='tab:green', label=\"MSE\")\n",
    "        p2, = par1.plot(params, quants[:,1], color='tab:blue', label=\"PSNR\")\n",
    "        p3, = par2.plot(params, quants[:,2], color='tab:orange', label=\"SSIM\")\n",
    "    if cm:\n",
    "        p4, = par3.plot(params, quants[:,3], color='tab:purple', label=\"CM\", linestyle='dashed', marker='x')\n",
    "\n",
    "    if cl:\n",
    "        host.yaxis.label.set_color(p1.get_color())\n",
    "        par1.yaxis.label.set_color(p2.get_color())\n",
    "        par2.yaxis.label.set_color(p3.get_color())\n",
    "        if cm:\n",
    "            par3.yaxis.label.set_color(p4.get_color())\n",
    "\n",
    "    lns = [p1, p2, p3]\n",
    "    if cm:\n",
    "        lns = [p1, p2, p3, p4]\n",
    "    host.legend(handles=lns, loc=legloc)\n",
    "\n",
    "    # plot optimum\n",
    "    if offset:\n",
    "        opti = get_optimum(quants, interval, offset=True)\n",
    "    else:\n",
    "        opti = get_optimum(quants, interval, offset=False)\n",
    "    host.axvline(x=opti, color = 'tab:red', linestyle='dashed')\n",
    "\n",
    "    host.set_xlabel(f'{xl}, optimal: {float(opti):.3}')\n",
    "\n",
    "    # right, left, top, bottom\n",
    "    par2.spines['right'].set_position(('outward', 60))\n",
    "\n",
    "    #plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "def get_optimum(array, interval=1, offset=True):\n",
    "    \"\"\" takes an array of MSE, PSNR and SSIM values and returns the index of the optimum values \"\"\"\n",
    "    \"\"\" mse_min = np.argmin(array[:,0])\n",
    "    psnr_max = np.argmax(array[:,1])\n",
    "    ssim_max = np.argmax(array[:,2])\n",
    "    print(mse_min,psnr_max,ssim_max) \"\"\"\n",
    "    #array[:,0] = -1*array[:,0]\n",
    "    #array[:,1] = array[:,1]/30\n",
    "    #summa = np.sum(array,axis=1).tolist()\n",
    "    #summa_max = np.argmax(summa)\n",
    "    #result = summa_max*interval\n",
    "    if offset:\n",
    "        return (np.argmax(array[:,3])+1)*interval\n",
    "    else:\n",
    "        return np.argmax(array[:,3])*interval\n",
    "    \n",
    "# data structure modification\n",
    "def batchify(input):\n",
    "    return tf.expand_dims(input,0)\n",
    "\n",
    "def blockshaped(arr, nrows, ncols):\n",
    "    # source: https://stackoverflow.com/a/16858283\n",
    "    \"\"\"\n",
    "    Return an array of shape (n, nrows, ncols) where\n",
    "    n * nrows * ncols = arr.size\n",
    "\n",
    "    If arr is a 2D array, the returned array should look like n subblocks with\n",
    "    each subblock preserving the \"physical\" layout of arr.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    assert h % nrows == 0, f\"{h} rows is not evenly divisible by {nrows}\"\n",
    "    assert w % ncols == 0, f\"{w} cols is not evenly divisible by {ncols}\"\n",
    "    return (arr.reshape(h//nrows, nrows, -1, ncols).swapaxes(1,2).reshape(-1, nrows, ncols))\n",
    "    \n",
    "def into_chunks(img, dim_goal=28):\n",
    "    \"\"\" function to cut up image into equally sized chunks with dimensions close to dim_goal \"\"\"\n",
    "    for i in range(dim_goal,((dim_goal*2)-1)):\n",
    "        dim = img.shape[0]/i\n",
    "        if dim.is_integer():\n",
    "            # if the image is divisible by 'dim' then we can cut it up into chunks and write this to a variable\n",
    "            print(f'Cutting up image into chunks of size: {int(i)}x{int(i)}')\n",
    "            img_chunks = blockshaped(img, int(i), int(i))\n",
    "            div_int = int(dim)\n",
    "            break\n",
    "    return img_chunks, div_int\n",
    "\n",
    "def unblockinator(chunk_list, hor_n, ver_n):\n",
    "    \"\"\" function to recombine a list of chunks made by 'into_chunks' \"\"\"\n",
    "    total_stack = []\n",
    "    for i in range(ver_n):\n",
    "        total_stack.append(np.hstack([x for x in chunk_list[i*hor_n:i*hor_n+hor_n]]))\n",
    "    total_stack = np.vstack(total_stack)\n",
    "    return total_stack\n",
    "\n",
    "def get_kernel(full, kernel, mode='normal', crop_px=5):\n",
    "    \"\"\" Get Kernel\n",
    "    A simple function which takes the center part of any array given in the first argument based on the size of the second argument\n",
    "    Allow the user to specify if they want the default behaviour, or a simple cropping function to crop the image by 'crop_px' pixels \"\"\"\n",
    "    if mode == 'normal':\n",
    "        fullx, fully = full.shape\n",
    "        kernelx, kernely = kernel.shape\n",
    "        startx, endx = int(np.floor((fullx-kernelx)/2)), int(np.floor((fullx-kernelx)/2)+kernelx)\n",
    "        starty, endy = int(np.floor((fully-kernely)/2)), int(np.floor((fully-kernelx)/2)+kernely)\n",
    "        return full[startx:endx,starty:endy]\n",
    "    elif mode == 'crop':\n",
    "        fullx, fully = full.shape\n",
    "        startx, endx = crop_px, fullx-crop_px\n",
    "        starty, endy = crop_px, fully-crop_px\n",
    "        return full[startx:endx,starty:endy]\n",
    "\n",
    "def reshape(curr, goal, pad_val=0):\n",
    "    \"\"\" this function take the psf and reshapes it to a size that matches that of the image \"\"\"\n",
    "    currx, curry = curr.shape\n",
    "    goalx, goaly = goal.shape\n",
    "    x_pad_affix = int(np.floor((goalx-currx)/2))\n",
    "    x_pad_suffix = int(goalx-currx-x_pad_affix)\n",
    "    y_pad_affix = int(np.floor((goaly-curry)/2))\n",
    "    y_pad_suffix = int(goaly-curry-y_pad_affix)\n",
    "    result = np.pad(curr, ((x_pad_affix, x_pad_suffix),(y_pad_affix, y_pad_suffix)), 'constant', constant_values=pad_val)\n",
    "    return result\n",
    "\n",
    "def disk_mask(img, fmod):\n",
    "    #source: https://stackoverflow.com/a/70283438\n",
    "    mask = np.zeros(img.shape, dtype=np.uint8)\n",
    "    radius = fmod\n",
    "    rr, cc = disk((0, 0), radius)\n",
    "    mask[rr, cc] = 1\n",
    "    return mask\n",
    "\n",
    "# data modification\n",
    "def normalize_image(img):\n",
    "    \"\"\" this function normalize the image to the range [0,1] \"\"\"\n",
    "    img = img.astype(float)\n",
    "    img = img - img.min()\n",
    "    img = img / (img.max() - img.min())\n",
    "    return img\n",
    "\n",
    "def conv_prod(a, b, fmod=False):\n",
    "    \"\"\" Convolution Product\n",
    "    Takes two arguments; a and b, and returns the convolution product of the two \"\"\"\n",
    "    if a.shape[0] > b.shape[0]:\n",
    "        if not fmod:\n",
    "            return ifft2(fft2(a)*fft2(b,s=(a.shape[0],a.shape[1]))).real\n",
    "        else:\n",
    "            c = fft2(a)*fft2(b,s=(a.shape[0],a.shape[1]))\n",
    "            # suppress frequencies radially starting at 'fmod'\n",
    "            c = c*disk_mask(c,fmod)\n",
    "            return ifft2(c).real\n",
    "    elif a.shape[0] < b.shape[0]:\n",
    "        return ifft2(fft2(b)*fft2(a,s=(b.shape[0],b.shape[1]))).real\n",
    "    else:\n",
    "        return ifft2(fft2(a)*fft2(b)).real\n",
    "        \n",
    "def normalize_complex_arr(a):\n",
    "    # source: https://stackoverflow.com/a/41576956\n",
    "    \"\"\" function to normalize complex arrays\"\"\"\n",
    "    a_oo = a - a.real.min() - 1j*a.imag.min() # origin offsetted\n",
    "    return a_oo/np.abs(a_oo).max()\n",
    "\n",
    "def matlab_style_gauss2D(shape=(15,15),sigma=1):\n",
    "    # source: https://stackoverflow.com/a/17201686\n",
    "    \"\"\" 2D gaussian mask - should give the same result as MATLAB's fspecial('gaussian',[shape],[sigma]) \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "# deconvolution schemes\n",
    "# Naive\n",
    "def decon_Naive(image, psf):\n",
    "    \"\"\" Naive Deconvolution\n",
    "    Takes two arguments; image and psf, and returns the deconvoluted result\n",
    "    image (array): should be the distorted image\n",
    "    psf (array): should be the point spread function \"\"\"\n",
    "    image_fourier = fft2(image)\n",
    "    psf_fourier = fft2(psf,s=image.shape)\n",
    "    return ifft2(image_fourier/psf_fourier).real\n",
    "\n",
    "# Adjusted Wiener\n",
    "def wiener_adj(image, psf, sigma=0.1):\n",
    "    return ifft2(fft2(wiener(image, psf, sigma))/fft2(psf,s=image.shape)).real\n",
    "    \n",
    "# Richardson-Lucy\n",
    "@timing\n",
    "def RL_default(image, psf, niter, mode='normal', crop=5, quants=False, fmod=False):\n",
    "    \"\"\" Richadson-Lucy Deconvolution\n",
    "    Takes three arguments: image, psf, iter and returns the deconvoluted result\n",
    "    image (array): should be the distorted image\n",
    "    psf (array): should be the point spread function\n",
    "    iter (integer): should be the number of Richadson-Lucy iterations to perform\n",
    "    mode (string): 'normal' results in normal use of the RL algorithm, 'self' results in a blind self-iterative process \"\"\"\n",
    "    if quants:\n",
    "        storage = []\n",
    "\n",
    "    if mode == 'normal':\n",
    "        base = image\n",
    "        for x in range(niter):\n",
    "            if not fmod:\n",
    "                image = fftc(base/(fftc(image,psf,'same')+1e-12),np.flip(psf),'same')*image\n",
    "            else:\n",
    "                image = conv_prod(base/(conv_prod(image,psf,fmod=fmod)+1e-12),np.flip(psf),fmod=fmod)*image\n",
    "            if quants:\n",
    "                storage.append(get_quant(image,f))\n",
    "    elif mode == 'crop':\n",
    "        image = get_kernel(image, psf, mode='crop', crop_px=crop)\n",
    "        f_crop = get_kernel(f, psf, mode='crop', crop_px=crop)\n",
    "        base = image\n",
    "        for x in range(niter):\n",
    "            image = fftc(base/(fftc(image,psf,'same')+1e-12),np.flip(psf),'same')*image\n",
    "            if quants:\n",
    "                storage.append(get_quant(image,f_crop))\n",
    "    elif mode == 'reflect':\n",
    "        base_original = image\n",
    "        base = np.pad(image, int(np.ceil(image.shape[0]*0.1)), mode='reflect')\n",
    "        image = base\n",
    "        for x in range(niter):\n",
    "            if not fmod:\n",
    "                image = fftc(base/(fftc(image,psf,'same')+1e-12),np.flip(psf),'same')*image\n",
    "            else:\n",
    "                image = conv_prod(base/(conv_prod(image,psf,fmod=fmod)+1e-12),np.flip(psf),fmod=fmod)*image\n",
    "            if quants:\n",
    "                result = get_kernel(image.real,base_original)\n",
    "                storage.append(get_quant(result,f))\n",
    "        image = get_kernel(image.real,base_original)\n",
    "    elif mode == 'crofl':\n",
    "        image = get_kernel(image, psf, mode='crop', crop_px=crop)\n",
    "        f_crop = get_kernel(f, psf, mode='crop', crop_px=crop)\n",
    "        base_original = image\n",
    "        base = np.pad(image, int(np.ceil(image.shape[0]*0.1)), mode='reflect')\n",
    "        image = base\n",
    "        for x in range(niter):\n",
    "            image = fftc(base/(fftc(image,psf,'same')+1e-12),np.flip(psf),'same')*image\n",
    "            if quants:\n",
    "                result = get_kernel(image.real,base_original)\n",
    "                storage.append(get_quant(result,f_crop))\n",
    "        image = get_kernel(image.real,base_original)\n",
    "    if quants:\n",
    "        storage = np.array(storage)\n",
    "        return image.real, storage\n",
    "    else:\n",
    "        return image.real\n",
    "\n",
    "# Richardson-Lucy using frequency suppression\n",
    "def RL_default_gibbs(iters=1, mode='normal', fmod=50):\n",
    "    result = RL_default(h,g,iters,mode=mode,fmod=fmod)\n",
    "    return result\n",
    "\n",
    "# Iterative Richardson-Lucy (iRL)\n",
    "def RL_f_loop(f, g, c, nr_iter, clip):\n",
    "    \"\"\" this function takes an initial f, g and c, and returns the new f after 'nr_iter' RL iterations \"\"\"\n",
    "    if g.shape != f.shape:\n",
    "        # reshape and center the psf to match the size of the image to make convolution possible\n",
    "        g = reshape(g, f)\n",
    "\n",
    "    for i in range(nr_iter):\n",
    "        #print('f',i)\n",
    "        f = fftc(c/(fftc(f,g,'same')+1e-12),np.flip(g),'same')*f\n",
    "        #f = normalize_complex_arr(f)\n",
    "        #f /= np.sum(f)\n",
    "    if clip:\n",
    "        return np.clip(np.abs(f),0,1)\n",
    "    else:\n",
    "        return f\n",
    "\n",
    "def RL_g_loop(f, g, c, nr_iter, clip):\n",
    "    \"\"\" this function takes an initial f, g and c, and returns the new g after 'nr_iter' RL iterations \"\"\"\n",
    "    if g.shape != f.shape:\n",
    "        # reshape and center the psf to match the size of the image to make convolution possible\n",
    "        g = reshape(g, f)\n",
    "\n",
    "    for i in range(nr_iter):\n",
    "        #print('g',i)\n",
    "        g = fftc(c/(fftc(g,f,'same')+1e-4),np.flip(f),'same')*g\n",
    "        #g = normalize_complex_arr(g)\n",
    "        #g /= np.sum(g)\n",
    "    return g\n",
    "\n",
    "@timing\n",
    "def RL_iter(f, g, c, internal, nr_iter, mode='normal', clip_f=False, clip_g=False, inter_crop=True, quants=False):\n",
    "    \"\"\" main Richardson-Lucy loop \"\"\"\n",
    "    i = 0\n",
    "    f_base = f\n",
    "    crop_kernel = np.zeros((15,15))\n",
    "    if quants:\n",
    "        storage = []\n",
    "    while i < nr_iter:\n",
    "        #print('loop',i)\n",
    "        #print(np.sum(f),np.max(f))\n",
    "        g = RL_g_loop(f, g, f_base, internal, clip_g)\n",
    "        f = RL_f_loop(f, g, f_base, internal, clip_f)\n",
    "        if inter_crop:\n",
    "            g = get_kernel(g,crop_kernel)\n",
    "        if quants:\n",
    "            result = get_kernel(f.real,c)\n",
    "            storage.append(get_quant(result,c))\n",
    "        i += 1\n",
    "    if quants:\n",
    "        storage = np.array(storage)\n",
    "        return f.real, g.real, storage\n",
    "    else:\n",
    "        return f.real, g.real\n",
    "\n",
    "print(f'Custom functions have been loaded.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at what image convolution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA, MODIFY AND PRESENT\n",
    "#----------------------------------------------------------------\n",
    "# We can load an image for further use from the 'skimage' library\n",
    "f = rgb2gray(data.astronaut()).astype('float32')\n",
    "\n",
    "# Now we can create a PSF or convolution kernel (with sum(g) = 1)\n",
    "g = np.ones((5,5))\n",
    "g /= np.sum(g)\n",
    "\n",
    "# Applying the convolution yields the following:\n",
    "h = conv_prod(f,g)\n",
    "\n",
    "# If we add a generic distortion in the form of random noise we get:\n",
    "random_mask = 0.03 * np.random.randn(h.shape[0],h.shape[1])\n",
    "f_noise = f + random_mask\n",
    "f_noise = normalize_image(f_noise)\n",
    "h_noise = h + random_mask\n",
    "h_noise = normalize_image(h_noise)\n",
    "# plot the images\n",
    "plot_images([\n",
    "        f, f_noise, h, h_noise\n",
    "    ],[\n",
    "        r'$f$', r'$f_{noise}$', r'$h$', r'$h_{noise}$'\n",
    "    ])\n",
    "# print quality materics for each image\n",
    "print_quant('ground-truth',f,f)\n",
    "print_quant('noisy image',f_noise,f)\n",
    "print_quant('blurry image',h,f)\n",
    "print_quant('noisy blurry image',h_noise,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also start with some simple deconvolution methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-BLIND NAIVE DECONVOLUTION: NO NOISE, BLUR\n",
    "#----------------------------------------------------------------\n",
    "# compute the naive deconvolution\n",
    "f_naive = decon_Naive(h,g)\n",
    "# plot original, modified and reconstructed\n",
    "plot_images([\n",
    "        f, h, f_naive\n",
    "    ],[\n",
    "        r'$f$', r'$h$', r'$f_{naive}$'\n",
    "    ])\n",
    "# print similarity mertics\n",
    "print_quant('naive deconvolution',f_naive,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-BLIND NAIVE DECONVOLUTION: NOISE, BLUR\n",
    "#----------------------------------------------------------------\n",
    "# compute the naive deconvolution\n",
    "f_naive = decon_Naive(h_noise,g)\n",
    "# plot original, modified and reconstructed\n",
    "plot_images([\n",
    "        f, h_noise, f_naive\n",
    "    ],[\n",
    "        r'$f$', r'$h_{noise}$', r'$f_{naive}$'\n",
    "    ])\n",
    "# print similarity mertics\n",
    "print_quant('naive method applied to noisy blurry image',f_naive,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a naive deconvolution we can turn to the Wiener method to combat noise artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: WIENER\n",
    "#----------------------------------------------------------------\n",
    "# computed the Wiener deconvolution\n",
    "wiener_01 = wiener(h_noise, g, 0.1)\n",
    "wiener_02 = wiener(h_noise, g, 0.2)\n",
    "wiener_05 = wiener(h_noise, g, 0.5)\n",
    "# plot original, modified and reconstructed\n",
    "plot_images([\n",
    "        f, h_noise, wiener_01\n",
    "    ],[\n",
    "        r'$f$', r'$h_{noise}$', r'$f_{Wiener~0.1}$'\n",
    "    ])\n",
    "# print similarity mertics\n",
    "print_quant('Wiener deconvolution (0.1)',wiener_01,f)\n",
    "print_quant('Wiener deconvolution (0.2)',wiener_02,f)\n",
    "print_quant('Wiener deconvolution (0.5)',wiener_05,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: WIENER --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_wiener(params=np.arange(0.1, 50, 0.5)):\n",
    "    # create a list to store the results in\n",
    "    storage = []\n",
    "    # append quant results to empty list\n",
    "    for i in params:\n",
    "        result = wiener(h_noise, g, i)\n",
    "        storage.append(get_quant(result,f))\n",
    "    # transform list to numpy array\n",
    "    storage = np.array(storage)\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(params, storage, title='Metrics of Wiener per sigma', xl='Lambda', interval=0.5, cm=True, optweak=True) # interval = 'params' arange interval\n",
    "\n",
    "graph_wiener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: ADJUSTED WIENER\n",
    "#----------------------------------------------------------------\n",
    "# computed the default and adjusted Wiener deconvolutions\n",
    "f_wiener_default = wiener(h_noise, g, 0.1)\n",
    "f_wiener_adjusted = wiener_adj(h_noise, g, 0.1)\n",
    "# plot original, modified and reconstructed\n",
    "plot_images([\n",
    "        h_noise, f_wiener_default, f_wiener_adjusted\n",
    "    ],[\n",
    "        r'$h_{noise}$', r'$f_{Wiener~3}$', r'$f_{Wiener~adjusted~3}$'\n",
    "    ])\n",
    "# print similarity mertics\n",
    "print_quant('default Wiener method',f_wiener_default,f)\n",
    "print_quant('adjusted Wiener method',f_wiener_adjusted,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: ADJUSTED WIENER --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_wiener_adj(params=np.arange(0.1, 50, 0.5)):\n",
    "    # create a list to store the results in\n",
    "    storage = []\n",
    "    # append quant results to empty list\n",
    "    for i in params:\n",
    "        result = wiener_adj(h_noise, g, i)\n",
    "        storage.append(get_quant(result,f))\n",
    "    # transform list to numpy array\n",
    "    storage = np.array(storage)\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(params, storage, title='Metrics of adjusted Wiener per sigma', xl='Lambda', interval=0.5, cm=True, optweak=True) # interval = 'params' arange interval\n",
    "\n",
    "graph_wiener_adj()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also account in part for uncertainty in our input parameters by using the unsupervised Wiener method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: UNSUPERVISED WIENER\n",
    "#----------------------------------------------------------------\n",
    "# calculate unsupervised wiener deconvolutions for different sigma valued gaussian psf's\n",
    "unsup_s1 = unsupervised_wiener(h_noise,matlab_style_gauss2D(sigma=1))[0]\n",
    "unsup_s2 = unsupervised_wiener(h_noise,matlab_style_gauss2D(sigma=2))[0]\n",
    "unsup_s3 = unsupervised_wiener(h_noise,matlab_style_gauss2D(sigma=3))[0]\n",
    "# plot the results\n",
    "plot_images([\n",
    "        unsup_s1, unsup_s2, unsup_s3\n",
    "    ],[\n",
    "        r'$\\sigma=1$', r'$\\sigma=2$', r'$\\sigma=3$'\n",
    "    ])\n",
    "# print similarity mertics\n",
    "print_quant('unsupervised Wiener method with sigma=1',unsup_s1,f)\n",
    "print_quant('unsupervised Wiener method with sigma=2',unsup_s2,f)\n",
    "print_quant('unsupervised Wiener method with sigma=3',unsup_s3,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: UNSUPERVISED WIENER --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_wiener_unsup(params=np.arange(0.1, 5, 0.1)):\n",
    "    # create a list to store the results in\n",
    "    storage = []\n",
    "    # append quant results to empty list\n",
    "    for i in params:\n",
    "        result = unsupervised_wiener(h_noise,matlab_style_gauss2D(sigma=i))[0]\n",
    "        storage.append(get_quant(result,f))\n",
    "    # transform list to numpy array\n",
    "    storage = np.array(storage)\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(params, storage, title='Metrics of unsupervised Wiener per sigma', xl='Sigma', interval=0.1, cm=True, optweak=True) # interval = 'params' arange interval\n",
    "\n",
    "graph_wiener_unsup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the results we can try the blind/semi-blind Richardson-Lucy deconvolution method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIND DECONVOLUTION: DEFAULT RICHARDSON-LUCY (NO NOISE, BLUR)\n",
    "#----------------------------------------------------------------\n",
    "# NOTE: the psf can be chosen to be either the same as the 'real' blur kernel, or a general gaussian.\n",
    "#       uncomment only whichever one you would like to use\n",
    "#_____ real blur kernel\n",
    "#g = np.ones((5,5))\n",
    "#g /= np.sum(g)\n",
    "#_____ gaussian\n",
    "g = matlab_style_gauss2D(shape=(15,15), sigma=2)\n",
    "def rl_gallery(noise=False, noise_level=0.1):\n",
    "    # calculate the default and custom RL deconvolutions at different numbers of iterations\n",
    "    if not noise:\n",
    "        rld_5 = richardson_lucy_t(h,g,5)\n",
    "        rld_100 = richardson_lucy_t(h,g,100)\n",
    "        rld_250 = richardson_lucy_t(h,g,250)\n",
    "        rlc_5 = RL_default(h,g,5)\n",
    "        rlc_100 = RL_default(h,g,100)\n",
    "        rlc_250 = RL_default(h,g,250)\n",
    "    else:\n",
    "        random_mask = noise_level * np.random.randn(h.shape[0],h.shape[1])\n",
    "        h_noise = h + random_mask\n",
    "        h_noise = normalize_image(h_noise)\n",
    "        rld_5 = richardson_lucy_t(h_noise,g,5)\n",
    "        rld_100 = richardson_lucy_t(h_noise,g,100)\n",
    "        rld_250 = richardson_lucy_t(h_noise,g,250)\n",
    "        rlc_5 = RL_default(h_noise,g,5)\n",
    "        rlc_100 = RL_default(h_noise,g,100)\n",
    "        rlc_250 = RL_default(h_noise,g,250)\n",
    "    # plot\n",
    "    plot_images([\n",
    "            f, rld_5, rld_100, rld_250, h, rlc_5, rlc_100, rlc_250\n",
    "        ],[\n",
    "            r'$f$', r'$RL_{5}$', r'$RL_{100}$', r'$RL_{250}$',\n",
    "            r'$h$', r'$RLc_{5}$', r'$RLc_{100}$', r'$RLc_{250}$'\n",
    "        ])\n",
    "    # print similarity mertics\n",
    "    print_quant('default RL at 5 iterations',rld_5,f)\n",
    "    print_quant('default RL at 100 iterations',rld_100,f)\n",
    "    print_quant('default RL at 250 iterations',rld_250,f)\n",
    "    print_quant('custom RL at 5 iterations',rlc_5,f)\n",
    "    print_quant('custom RL at 100 iterations',rlc_100,f)\n",
    "    print_quant('custom RL at 250 iterations',rlc_250,f)\n",
    "\n",
    "rl_gallery(noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON SPEED scikit RL AND CUSTOM RL FUNCTIONS\n",
    "#----------------------------------------------------------------\n",
    "#single(richardson_lucy_t(h_noise,g,250))\n",
    "#RL_default(h_noise,g,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT HIGHLIGHTS (CHOOSE ONE): DEFAULT RICHARDSON-LUCY (NO NOISE, BLUR)\n",
    "#----------------------------------------------------------------\n",
    "# uncomment only the selection which you want to see\n",
    "#single(rld_250, 100, 0, 0)\n",
    "#single(rlc_250, 100, 0, 0)\n",
    "#single(rld_250, 100, 100, 100)\n",
    "#single(rlc_250, 100, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: RICHARDSON-LUCY DEFAULT --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_rl_defult(params=250, noise=False):\n",
    "    # create a list to store the results in\n",
    "    if not noise:\n",
    "        result = RL_default(h,g,params,mode='normal',quants=True)[1]\n",
    "    else:\n",
    "        result = RL_default(h_noise,g,params,mode='normal',quants=True)[1]\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(np.arange(0, params, 1), result, title=None, xl='Iterations', cm=False, optweak=False, offset=False)\n",
    "\n",
    "graph_rl_defult(noise=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize artifacting in the result image modifications can be applied and the default RL method can be modified, starting with reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIND DECONVOLUTION: REFLECTIVE PADDING\n",
    "#----------------------------------------------------------------\n",
    "# produce a reflected exmaple of the blurred image\n",
    "h_refl = np.pad(h, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "def rl_refl_gallery(noise=False, noise_level=0.1):\n",
    "    # calculate the reflected custom RL deconvolutions at different numbers of iterations\n",
    "    if not noise:\n",
    "        rlc_5_r = RL_default(h,g,5,mode='reflect')\n",
    "        rlc_100_r = RL_default(h,g,100,mode='reflect')\n",
    "        rlc_250_r = RL_default(h,g,250,mode='reflect')\n",
    "        # plot\n",
    "        plot_images([\n",
    "                h, rlc_5_r, rlc_100_r, rlc_250_r\n",
    "            ],[\n",
    "                r'$h$', r'$f_{RL~reflect~5}$', r'$f_{RL~reflect~100}$', r'$f_{RL~reflect~250}$'\n",
    "            ])\n",
    "    else:\n",
    "        random_mask = noise_level * np.random.randn(h.shape[0],h.shape[1])\n",
    "        h_noise = h + random_mask\n",
    "        h_noise = normalize_image(h_noise)\n",
    "        rlc_5_r = RL_default(h_noise,g,5,mode='reflect')\n",
    "        rlc_100_r = RL_default(h_noise,g,100,mode='reflect')\n",
    "        rlc_250_r = RL_default(h_noise,g,250,mode='reflect')\n",
    "        # plot\n",
    "        plot_images([\n",
    "                h_noise, rlc_5_r, rlc_100_r, rlc_250_r\n",
    "            ],[\n",
    "                r'$h_{noise}$', r'$f_{RL~reflect~5}$', r'$f_{RL~reflect~100}$', r'$f_{RL~reflect~250}$'\n",
    "            ])\n",
    "    # print similarity mertics\n",
    "    print_quant('reflective RL at 5 iterations',rlc_5_r,f)\n",
    "    print_quant('reflective RL at 100 iterations',rlc_100_r,f)\n",
    "    print_quant('reflective RL at 250 iterations',rlc_250_r,f)\n",
    "\n",
    "rl_refl_gallery(noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT HIGHLIGHTS (CHOOSE ONE): REFLECTIVE RICHARDSON-LUCY (NO NOISE, BLUR)\n",
    "#----------------------------------------------------------------\n",
    "# uncomment only the selection which you want to see\n",
    "#single(rlc_5_r, 100, 0, 0)\n",
    "#single(rlc_100_r, 100, 0, 0)\n",
    "#single(rlc_250_r, 100, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: REFLECTIVE RL --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_rl_refl(params=250):\n",
    "    # create a list to store the results in\n",
    "    result = RL_default(h,g,params,mode='reflect',quants=True)[1]\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(np.arange(0, params, 1), result, title='Metrics of reflective RL over iterations', xl='Iterations')\n",
    "\n",
    "graph_rl_refl()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to further improve a crop can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIND DECONVOLUTION: CROPPED\n",
    "#----------------------------------------------------------------\n",
    "def rl_crop_gallery(noise=False, noise_level=0.1):\n",
    "    # calculate the cropped custom RL deconvolutions at different numbers of iterations\n",
    "    if not noise:\n",
    "        rlc_5_c = RL_default(h,g,5,mode='crop')\n",
    "        rlc_100_c = RL_default(h,g,100,mode='crop')\n",
    "        rlc_250_c = RL_default(h,g,250,mode='crop')\n",
    "        # plot\n",
    "        plot_images([\n",
    "                h, rlc_5_c, rlc_100_c, rlc_250_c\n",
    "            ],[\n",
    "                r'$h$', r'$f_{RL~crop~5}$', r'$f_{RL~crop~100}$', r'$f_{RL~crop~250}$'\n",
    "            ])\n",
    "    else:\n",
    "        random_mask = noise_level * np.random.randn(h.shape[0],h.shape[1])\n",
    "        h_noise = h + random_mask\n",
    "        h_noise = normalize_image(h_noise)\n",
    "        rlc_5_c = RL_default(h_noise,g,5,mode='crop')\n",
    "        rlc_100_c = RL_default(h_noise,g,100,mode='crop')\n",
    "        rlc_250_c = RL_default(h_noise,g,250,mode='crop')\n",
    "        # plot\n",
    "        plot_images([\n",
    "                h_noise, rlc_5_c, rlc_100_c, rlc_250_c\n",
    "            ],[\n",
    "                r'$h_{noise}$', r'$f_{RL~crop~5}$', r'$f_{RL~crop~100}$', r'$f_{RL~crop~250}$'\n",
    "            ])\n",
    "    # print similarity mertics\n",
    "    f_crop = get_kernel(f,g,mode='crop')\n",
    "    print_quant('cropped RL at 5 iterations',rlc_5_c,f_crop)\n",
    "    print_quant('cropped RL at 100 iterations',rlc_100_c,f_crop)\n",
    "    print_quant('cropped RL at 250 iterations',rlc_250_c,f_crop)\n",
    "\n",
    "rl_crop_gallery(noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT HIGHLIGHTS (CHOOSE ONE): CROPPED RICHARDSON-LUCY (NO NOISE, BLUR)\n",
    "#----------------------------------------------------------------\n",
    "# uncomment only the selection which you want to see\n",
    "#single(rlc_5_c, 100, 0, 0)\n",
    "#single(rlc_100_c, 100, 0, 0)\n",
    "#single(rlc_250_c, 100, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: CROPPED RL --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_rl_crop(params=250):\n",
    "    # create a list to store the results in\n",
    "    result = RL_default(h,g,params,mode='crop',quants=True)[1]\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(np.arange(0, params, 1), result, xl='Iterations')\n",
    "\n",
    "graph_rl_crop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the crop with a subsequent reflective padding even further improves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: CROPPED REFLECTIVE PADDING\n",
    "#----------------------------------------------------------------\n",
    "# produce a cropped reflected example of the blurred image\n",
    "h_crop = get_kernel(h,g,mode='crop')\n",
    "h_crop_refl = np.pad(h_crop, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "\n",
    "def rl_crop_refl_gallery(noise=False, noise_level=0.1):\n",
    "    # calculate the reflected custom RL deconvolutions at different numbers of iterations\n",
    "    if not noise:\n",
    "        h_crop = get_kernel(h,g,mode='crop')\n",
    "        rlc_5_cr = RL_default(h_crop,g,5,mode='reflect')\n",
    "        rlc_100_cr = RL_default(h_crop,g,100,mode='reflect')\n",
    "        rlc_250_cr = RL_default(h_crop,g,250,mode='reflect')\n",
    "        # plot\n",
    "        plot_images([\n",
    "                h_crop_refl, rlc_5_cr, rlc_100_cr, rlc_250_cr\n",
    "            ],[\n",
    "                r'$h_{crop+reflect}$', r'$RL_{crop+reflect~5}$', r'$RL_{crop+reflect~100}$', r'$RL_{crop+reflect~250}$'\n",
    "            ])\n",
    "    else:\n",
    "        random_mask = noise_level * np.random.randn(h.shape[0],h.shape[1])\n",
    "        h_noise = h + random_mask\n",
    "        h_noise = normalize_image(h_noise)\n",
    "        h_crop = get_kernel(h_noise,g,mode='crop')\n",
    "        h_crop_refl = np.pad(h_crop, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "        rlc_5_cr = RL_default(h_crop,g,5,mode='reflect')\n",
    "        rlc_100_cr = RL_default(h_crop,g,100,mode='reflect')\n",
    "        rlc_250_cr = RL_default(h_crop,g,250,mode='reflect')\n",
    "        # plot\n",
    "        plot_images([\n",
    "                h_crop_refl, rlc_5_cr, rlc_100_cr, rlc_250_cr\n",
    "            ],[\n",
    "                r'$h_{crop+reflect noise}$', r'$RL_{crop+reflect~5}$', r'$RL_{crop+reflect~100}$', r'$RL_{crop+reflect~250}$'\n",
    "            ])\n",
    "    # print similarity mertics\n",
    "    # NOTE: to perform this comparison both images need to be of the same dimensions, so we need to crop the ground-truth as well\n",
    "    f_crop = get_kernel(f,g,mode='crop')\n",
    "    print_quant('cropped reflective RL at 5 iterations',rlc_5_cr,f_crop)\n",
    "    print_quant('cropped reflective RL at 100 iterations',rlc_100_cr,f_crop)\n",
    "    print_quant('cropped reflective RL at 250 iterations',rlc_250_cr,f_crop)\n",
    "\n",
    "rl_crop_refl_gallery(noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT HIGHLIGHTS (CHOOSE ONE): CROPPED REFLECTIVE RICHARDSON-LUCY (NO NOISE, BLUR)\n",
    "#----------------------------------------------------------------\n",
    "# uncomment only the selection which you want to see\n",
    "#single(rlc_5_cr, 100, 0, 0)\n",
    "#single(rlc_100_cr, 100, 0, 0)\n",
    "#single(rlc_250_cr, 100, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT HIGHLIGHTS (CHOOSE ONE): EXTREMELY HIGH NUMBER OF ITERATIONS\n",
    "# NOTE: high numbers of iterations significantly increase performance requirements\n",
    "#----------------------------------------------------------------\n",
    "# uncomment whichever number of RL deconvolution iteration steps you want to perform\n",
    "#rlc_1000_cr = RL_default(h_crop,g,1000,mode='reflect')\n",
    "#rlc_2500_cr = RL_default(h_crop,g,2500,mode='reflect')\n",
    "#rlc_var_cr = RL_default(h_crop,g,X,mode='reflect') # replace 'X' with the desired number of iterations\n",
    "# uncomment only the selection which you want to see\n",
    "#single(rlc_1000_cr, 100)\n",
    "#single(rlc_2500_cr, 100)\n",
    "#single(rlc_var_cr, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMI-BLIND DECONVOLUTION: CROPPED REFELCTIVE RL --> GRAPH\n",
    "#----------------------------------------------------------------\n",
    "def graph_rl_cr(params=250):\n",
    "    # create a list to store the results in\n",
    "    result = RL_default(h,g,params,mode='crofl',quants=True)[1]\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(np.arange(0, params, 1), result, xl='Iterations')\n",
    "\n",
    "graph_rl_cr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the iterative Richardson-Lucy method, including the previously mentioned improvements. Clipping the pixel values at a maximum to prevent artificial darkening helps with visual clarity in the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIND DECONVOLUTION: ITERATIVE RICHARDSON-LUCY (iRL), CROPPED REFLECTIVE PADDING WITH CLIPPING\n",
    "#----------------------------------------------------------------\n",
    "def irl_gallery(noise=False, noise_level=0.1):\n",
    "    # define constants\n",
    "    base_original = h\n",
    "    psf = matlab_style_gauss2D(shape=(15,15), sigma=2)\n",
    "    if not noise:\n",
    "        # compute both the clipped and non-clipped iRL deconvolution results\n",
    "        irl_blind_clip_3_5 = get_kernel(RL_iter(h_crop_refl,psf,f,3,5,clip_f=True)[0],base_original)\n",
    "        irl_blind_clip_3_15 = get_kernel(RL_iter(h_crop_refl,psf,f,3,15,clip_f=True)[0],base_original)\n",
    "        irl_blind_noclip_3_5 = get_kernel(RL_iter(h_crop_refl,psf,f,3,5,clip_f=False)[0],base_original)\n",
    "        irl_blind_noclip_3_15 = get_kernel(RL_iter(h_crop_refl,psf,f,3,15,clip_f=False)[0],base_original)\n",
    "        # plot\n",
    "        plot_images([\n",
    "                irl_blind_clip_3_5, irl_blind_clip_3_15, irl_blind_noclip_3_5, irl_blind_noclip_3_15\n",
    "            ],[\n",
    "                r'$i=3,k=5, clip$', r'$i=3,k=15, clip$', r'$i=3,k=5, noclip$', r'$i=3,k=15, noclip$'\n",
    "            ])\n",
    "    else:\n",
    "        random_mask = noise_level * np.random.randn(h.shape[0],h.shape[1])\n",
    "        h_noise = h + random_mask\n",
    "        h_noise = normalize_image(h_noise)\n",
    "        h_crop = get_kernel(h_noise,g,mode='crop')\n",
    "        h_crop_refl = np.pad(h_crop, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "        # compute both the clipped and non-clipped iRL deconvolution results\n",
    "        irl_blind_clip_3_5 = get_kernel(RL_iter(h_crop_refl,psf,f,3,5,clip_f=True)[0],base_original)\n",
    "        irl_blind_clip_3_15 = get_kernel(RL_iter(h_crop_refl,psf,f,3,15,clip_f=True)[0],base_original)\n",
    "        irl_blind_noclip_3_5 = get_kernel(RL_iter(h_crop_refl,psf,f,3,5,clip_f=False)[0],base_original)\n",
    "        irl_blind_noclip_3_15 = get_kernel(RL_iter(h_crop_refl,psf,f,3,15,clip_f=False)[0],base_original)\n",
    "        # plot\n",
    "        plot_images([\n",
    "                irl_blind_clip_3_5, irl_blind_clip_3_15, irl_blind_noclip_3_5, irl_blind_noclip_3_15\n",
    "            ],[\n",
    "                r'$i=3,k=5, clip~noise$', r'$i=3,k=15, clip~noise $', r'$i=3,k=5, noclip~noise$', r'$i=3,k=15, noclip~noise$'\n",
    "            ])\n",
    "    # print similarity mertics\n",
    "    # NOTE: to perform this comparison both images need to be of the same dimensions, so we need to crop the ground-truth as well\n",
    "    f_crop = get_kernel(f,g,mode='crop')\n",
    "    print_quant('cropped reflective iRL at 3 internal and 5 total iterations (clip)',irl_blind_clip_3_5,f)\n",
    "    print_quant('cropped reflective iRL at 3 internal and 15 total iterations (clip)',irl_blind_clip_3_15,f)\n",
    "    print_quant('cropped reflective iRL at 3 internal and 5 total iterations (no clip)',irl_blind_noclip_3_5,f)\n",
    "    print_quant('cropped reflective iRL at 3 internal and 15 total iterations (no clip)',irl_blind_noclip_3_15,f)\n",
    "\n",
    "irl_gallery(noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIND DECONVOLUTION: CROPPED REFELCTIVE iRL --> GRAPH (no noise)\n",
    "#----------------------------------------------------------------\n",
    "def graph_irl_cr(params=250):\n",
    "    random_mask = noise_level * np.random.randn(h.shape[0],h.shape[1])\n",
    "    h_noise = h + random_mask\n",
    "    h_noise = normalize_image(h_noise)\n",
    "    h_crop = get_kernel(h_noise,g,mode='crop')\n",
    "    h_crop_refl = np.pad(h_crop, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "    # create a list to store the results in\n",
    "    result = RL_iter(h_crop_refl,psf,f,3,params,clip_f=True,quants=True)[2]\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(np.arange(0, params, 1), result, xl='Iterations', legloc=6)\n",
    "\n",
    "graph_irl_cr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" random_mask = 0.03 * np.random.randn(h.shape[0],h.shape[1])\n",
    "h_noise = h + random_mask\n",
    "h_noise = normalize_image(h_noise)\n",
    "h_crop = get_kernel(h_noise,g,mode='crop')\n",
    "h_crop_refl = np.pad(h_crop, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "ok = RL_iter(h_crop_refl,psf,f,3,70,clip_f=True,clip_g=True)\n",
    "single(ok[1]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single(get_kernel(ok[0],base_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BLIND DECONVOLUTION: ITERATIVE RICHARDSON-LUCY (iRL), CROPPED REFLECTIVE PADDING WITH CLIPPING\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# define constants\n",
    "base_original = h_noise\n",
    "h_c_noise = get_kernel(h_noise,g,mode='crop')\n",
    "h_cr_noise = np.pad(h_c_noise, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "psf = matlab_style_gauss2D(shape=(15,15), sigma=2)\n",
    "# compute both the clipped and non-clipped iRL deconvolution results\n",
    "irl_cr_noise_3_5 = get_kernel(RL_iter(h_cr_noise,psf,f,3,5,clip_f=True)[0],base_original)\n",
    "irl_cr_noise_3_15 = get_kernel(RL_iter(h_cr_noise,psf,f,3,15,clip_f=True)[0],base_original)\n",
    "irl_cr_noise_3_5_nc = get_kernel(RL_iter(h_cr_noise,psf,f,3,5,clip_f=False)[0],base_original)\n",
    "irl_cr_noise_3_15_nc = get_kernel(RL_iter(h_cr_noise,psf,f,3,15,clip_f=False)[0],base_original)\n",
    "# plot\n",
    "plot_images([\n",
    "        irl_cr_noise_3_5, irl_cr_noise_3_15, irl_cr_noise_3_5_nc, irl_cr_noise_3_15_nc\n",
    "    ],[\n",
    "        r'$i=3,k=5, clip$', r'$i=3,k=15, clip$', r'$i=3,k=5, noclip$', r'$i=3,k=15, noclip$'\n",
    "    ])\n",
    "# print similarity mertics\n",
    "# NOTE: to perform this comparison both images need to be of the same dimensions, so we need to crop the ground-truth as well\n",
    "f_crop = get_kernel(f,g,mode='crop')\n",
    "print_quant('cropped reflective iRL at 3 internal and 5 total iterations (clip)',irl_cr_noise_3_5,f)\n",
    "print_quant('cropped reflective iRL at 3 internal and 15 total iterations (clip)',irl_cr_noise_3_15,f)\n",
    "print_quant('cropped reflective iRL at 3 internal and 5 total iterations (no clip)',irl_cr_noise_3_5_nc,f)\n",
    "print_quant('cropped reflective iRL at 3 internal and 15 total iterations (no clip)',irl_cr_noise_3_15_nc,f) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIND DECONVOLUTION: CROPPED REFELCTIVE iRL --> GRAPH (noise)\n",
    "#----------------------------------------------------------------\n",
    "def graph_irl_cr_noise(params=150):\n",
    "    h_c_noise = get_kernel(h_noise,g,mode='crop')\n",
    "    h_cr_noise = np.pad(h_c_noise, int(np.ceil(h.shape[0]*0.1)), mode='reflect')\n",
    "    # create a list to store the results in\n",
    "    result = RL_iter(h_cr_noise,psf,f,3,params,clip_f=True,quants=True)[2]\n",
    "    # call the plot function for the list of parameters and the quant array\n",
    "    plot_quants(np.arange(0, params, 1), result, xl='Iterations', legloc=6)\n",
    "\n",
    "graph_irl_cr_noise()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA (RUN ONCE)\n",
    "#----------------------------------------------------------------\n",
    "def load_data(dataset='mnist'):\n",
    "    if dataset == 'mnist':\n",
    "        # load data from mnist dataset\n",
    "        (train, _), (test, _) = mnist.load_data()\n",
    "        # we should also scale/normalize our data\n",
    "        train = train.reshape([-1,28,28,1]) / 255\n",
    "        test = test.reshape([-1,28,28,1]) / 255\n",
    "        #print(f'train shape = {train.shape}, test shape = {test.shape}')\n",
    "        return train, test\n",
    "    elif dataset == 'fashionmnist':\n",
    "        (train_f, _), (test_f, _) = fashion_mnist.load_data()\n",
    "        (train_m, _), (test_m, _) = mnist.load_data()\n",
    "        # we should also scale/normalize our data\n",
    "        train_f = train_f.reshape([-1,28,28,1]) / 255\n",
    "        test_f = test_f.reshape([-1,28,28,1]) / 255\n",
    "        train_m = train_m.reshape([-1,28,28,1]) / 255\n",
    "        test_m = test_m.reshape([-1,28,28,1]) / 255\n",
    "        # now we can concatenate our sets\n",
    "        train = np.concatenate([train_f,train_m])\n",
    "        test = np.concatenate([test_f,test_m])\n",
    "        #print(f'train shape = {train.shape}, test shape = {test.shape}')\n",
    "        return train, test\n",
    "    elif dataset == 'cifar10':\n",
    "        (train, _), (test, _) = cifar10.load_data()\n",
    "        # we should also scale/normalize our data\n",
    "        train = np.asarray([cv2.cvtColor(i, cv2.COLOR_BGR2GRAY) for i in train])\n",
    "        test = np.asarray([cv2.cvtColor(i, cv2.COLOR_BGR2GRAY) for i in test])\n",
    "        train = train.reshape([-1,32,32,1]) / 255\n",
    "        test = test.reshape([-1,32,32,1]) / 255\n",
    "        #print(f'train shape = {train.shape}, test shape = {test.shape}')\n",
    "        return train, test\n",
    "    elif dataset == 'lfw':\n",
    "        # NOTE: image selection HEAVILY impacts performance, select only as many images as your hardware allows\n",
    "        images = np.asarray([cv2.imread(file, 0) for file in glob.glob(\"./data/lfw/[a-c]*/*\")])\n",
    "        images = np.asarray([cv2.resize(i, (256, 256)) for i in images])\n",
    "        split_percentage = math.ceil(len(images)*0.8)\n",
    "        train = images[:split_percentage]\n",
    "        test = images[split_percentage:]\n",
    "        #train = np.asarray([tf.expand_dims(i,2) for i in train]) / 255\n",
    "        train = train.reshape([-1,256,256,1]) / 255\n",
    "        test = test.reshape([-1,256,256,1]) / 255\n",
    "        #print(f'train shape = {train.shape}, test shape = {test.shape}')\n",
    "        return train, test\n",
    "    \n",
    "\n",
    "train, test = load_data(dataset='mnist')\n",
    "\n",
    "# with the following we display the dimensions of the split dataset\n",
    "print(f'The training subset has size: {train.shape[0]}, and the test subset has size: {test.shape[0]}.')\n",
    "print(f'Each of the images in these subsets have dimensions: {train.shape[1:]}.')\n",
    "print(f'The range of values in each of the images/arrays in these subsets is: [{train[0].min()}, {train[0].max()}].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE CLEAN IMAGES\n",
    "#----------------------------------------------------------------\n",
    "sample_train_clean = []\n",
    "rows = 2 # \"\"\" variable (default=3) \"\"\"\n",
    "sample_train_list = range(4*rows)\n",
    "for i in sample_train_list:\n",
    "    sample_train_clean.append(train[i])\n",
    "plot_images(sample_train_clean, [f'train[{x}]' for x in sample_train_list], colormap='OrRd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and sample blurred images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD BLUR (RUN ONCE)\n",
    "#----------------------------------------------------------------\n",
    "# we can create a simple blur kernel, normalize it and match the dimensions to the dataset\n",
    "# note that generally odd integer dimensions are preferred\n",
    "blur_kernel = np.ones(shape=(5,5)) # \"\"\" variable (default=np.ones(shape=(5,5))) \"\"\"\n",
    "blur_kernel /= np.sum(blur_kernel)\n",
    "blur_kernel = np.atleast_3d(blur_kernel)\n",
    "# now we can create two empty arrays to be filled with our blurred images\n",
    "train_blur = [fftc(train[i], blur_kernel, 'same') for i in range(len(train))]\n",
    "test_blur = [fftc(test[i], blur_kernel, 'same') for i in range(len(test))]\n",
    "\n",
    "train_blur = np.asarray(train_blur)\n",
    "test_blur = np.asarray(test_blur)\n",
    "\n",
    "print(f'Blurred copy of dataset has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE BLURRED IMAGES\n",
    "#----------------------------------------------------------------\n",
    "sample_train_blur = []\n",
    "sample_train_blur_number = 4 # \"\"\" variable (default=4) \"\"\"\n",
    "for i in range(sample_train_blur_number):\n",
    "    sample_train_blur.append(train[i])\n",
    "    sample_train_blur.append(train_blur[i])\n",
    "\n",
    "plot_images(sample_train_blur,['train','blur']*sample_train_blur_number, colormap='OrRd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and sample noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD NOISE (RUN ONCE)\n",
    "#----------------------------------------------------------------\n",
    "# we can define the level of random noise we want in our images and apply it to each of our images\n",
    "noise = 0.3 # \"\"\" variable (default=0.3) \"\"\"\n",
    "train_noise = train + noise * np.random.normal(0, 1, size=train.shape)\n",
    "test_noise = test + noise * np.random.normal(0, 1, size=test.shape)\n",
    "\n",
    "train_noise = np.asarray(train_noise)\n",
    "test_noise = np.asarray(test_noise)\n",
    "\n",
    "print(f'Noisy copy of dataset has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE NOISY IMAGES\n",
    "#----------------------------------------------------------------\n",
    "sample_train_noise = []\n",
    "sample_train_noise_number = 4 # \"\"\" variable (default=4) \"\"\"\n",
    "for i in range(sample_train_noise_number):\n",
    "    sample_train_noise.append(train[i])\n",
    "    sample_train_noise.append(train_noise[i])\n",
    "\n",
    "plot_images(sample_train_noise,['train','noise']*sample_train_noise_number, colormap='OrRd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and sample noisy blurred images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD NOISE TO BLUR (RUN ONCE)\n",
    "#----------------------------------------------------------------\n",
    "# we can define the level of random noise we want in our images and apply it to each of our images\n",
    "noise = 0.03 # \"\"\" variable (default=0.3) \"\"\"\n",
    "train_blur_noise = train_blur + noise * np.random.normal(0, 1, size=train.shape)\n",
    "test_blur_noise = test_blur + noise * np.random.normal(0, 1, size=test.shape)\n",
    "\n",
    "train_blur_noise = np.asarray(train_blur_noise)\n",
    "test_blur_noise = np.asarray(test_blur_noise)\n",
    "\n",
    "print(f'Noisy copy of blurred dataset has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE NOISY BLURRED IMAGES\n",
    "#----------------------------------------------------------------\n",
    "sample_train_blur_noise = []\n",
    "sample_train_blur_noise_number = 4 # \"\"\" variable (default=4) \"\"\"\n",
    "for i in range(sample_train_blur_noise_number):\n",
    "    sample_train_blur_noise.append(train[i])\n",
    "    sample_train_blur_noise.append(train_blur_noise[i])\n",
    "\n",
    "plot_images(sample_train_blur_noise,['train','blur+noise']*sample_train_blur_noise_number, colormap='OrRd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct Machine Learning models and summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING OUR MODELS AND DEFINE LATEST MODEL TRAINING SET\n",
    "#----------------------------------------------------------------\n",
    "def model_simple():\n",
    "    mod = Sequential([\n",
    "        Input(shape=(None,None,1), name='input'),\n",
    "        # then we apply a 2D convolution with a ReLu activation applied to its output\n",
    "        Conv2D(32, 3, activation='relu', padding='same', name='Conv2D-0'),\n",
    "        # then we apply a max-pooling, to decrease the size of our intermediate layers and speed up the learning process\n",
    "        MaxPool2D(name='MaxPool2D-0'),\n",
    "        # as a regularization layer we use a simple dropout instead of an L2 regularization to prevent overfitting\n",
    "        #Dropout(0.3, name='Dropout-0'),\n",
    "        # the following layers are the same as before\n",
    "        Conv2D(32, 3, activation='relu', padding='same', name='Conv2D-1'),\n",
    "        MaxPool2D(name='MaxPool2D-1'),\n",
    "        Conv2D(32, 3, activation='relu', padding='same', name='Conv2D-2'),\n",
    "        # to get back a representation with the same dimensions we need to upsample\n",
    "        UpSampling2D(name='UpSampling2D-0'),\n",
    "        #Dropout(0.3, name='Dropout-1'),\n",
    "        Conv2D(32, 3, activation='relu', padding='same', name='Conv2D-3'),\n",
    "        UpSampling2D(name='UpSampling2D-1'),\n",
    "        # finally we can use a sigmoid activation 2D convolution layer to output our estimated de-noised and de-blurred image\n",
    "        Conv2D(1, 3, activation='sigmoid', padding='same', name='Conv2D-4')\n",
    "    ])\n",
    "\n",
    "    return mod, 'simple'\n",
    "\n",
    "def skip_model():\n",
    "    inputs = Input(shape=(None, None, 1))\n",
    "    # encoder\n",
    "    conv1 = Conv2D(32, 3, 1, padding='same')(inputs)\n",
    "    conv2 = Conv2D(64, 3, 1, activation = 'relu', padding = 'same')(conv1)\n",
    "    conv3 = Conv2D(128, 3, 1, activation = 'relu', padding = 'same')(conv2)\n",
    "    # decoder\n",
    "    up1 = Conv2D(128, 3, activation = 'relu', padding = 'same')(UpSampling2D(size = 1)(conv3))\n",
    "    # now we use a skip connection to return `merge1` as being `inputs + conv1`\n",
    "    skip1 = Add()([conv3,up1])\n",
    "    up2 = Conv2D(64, 3, activation = 'relu', padding = 'same')(UpSampling2D(size = 1)(skip1))\n",
    "    skip2 = Add()([conv2,up2])\n",
    "    up3 = Conv2D(32, 3, activation = 'relu', padding = 'same')(UpSampling2D(size = 1)(skip2))\n",
    "    skip3 = Add()([conv1,up3])\n",
    "    # lastly we run a convolution to return a result in the correct shape\n",
    "    norm_conv = Conv2D(1, 3, activation='sigmoid', padding='same')(up3)\n",
    "\n",
    "    mod = Model(inputs=inputs, outputs=norm_conv)\n",
    "    \n",
    "    return mod, 'skip'\n",
    "\n",
    "model, model_name = model_simple()\n",
    "\n",
    "model.compile(\n",
    "    # the adam optimizer is the standard and seems to be working well\n",
    "    # feel free to change to taste and experiment\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "latest_model = 'none'\n",
    "\n",
    "print(f'Model \"{model_name}\" has been selected, and latest training set used model is {latest_model}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARIZE CHOSEN MODEL\n",
    "#----------------------------------------------------------------\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a 'curriculum' for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TRAINING FUNCTION\n",
    "# When using logging (log=True) a TensorBoard log will be written to the 'logs' folder.\n",
    "#   Using the 'opentensorboard.bat' file will automatically open from the correct folder,\n",
    "#   Changing the file structure requires running a custom TensorBoard command to open the software and read the logs from the correct directory.\n",
    "# --------------------------------------------------------------\n",
    "def train_model(name='blur_noise', log=False, train_in=train_blur_noise, train_target=train, test_in=test_blur_noise, test_target=test, epochs=10, batch_size=128):\n",
    "    \"\"\" Train model on training data. \n",
    "    name = name of the model for logging purposes\n",
    "    log = boolean indicating if the model should log to file\n",
    "    train_in = the training data of the perturbed images\n",
    "    train_target = the corresponding target data of clean images\n",
    "    test_in = the test data of the perturbed images\n",
    "    test_target = the corresponding target data of clean images\n",
    "    epochs = the number of epochs to run model for\n",
    "    batch_size = the number of images in a batch (adjust as needed for dataset) \"\"\"\n",
    "\n",
    "    if log:\n",
    "        # initialize model tensorboard log\n",
    "        log_name = f'{name}_{int(time.time())}'\n",
    "        tensorboard = TensorBoard(log_dir=f'logs/{log_name}')\n",
    "\n",
    "        # train model on noisy blurred images\n",
    "        history = model.fit(\n",
    "                        train_in,\n",
    "                        train_target,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(test_in, test_target),\n",
    "                        callbacks=[tensorboard]\n",
    "                    )\n",
    "    else:\n",
    "        # train model on noisy blurred images\n",
    "        history = model.fit(\n",
    "                        train_in,\n",
    "                        train_target,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(test_in, test_target)\n",
    "                    )\n",
    "\n",
    "    print(f'Model has been trained. \\n Please reconsider before running this function again, as it can prove exceedingly resource exhaustive. ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model and run predictions (noise+blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING SELECTED MODEL (noise+blur)\n",
    "#----------------------------------------------------------------\n",
    "if latest_model != 'blur_noise':\n",
    "    train_model()\n",
    "    latest_model = 'blur_noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN PREDICTION AND SAMPLE RESULTS (noise+blur)\n",
    "#----------------------------------------------------------------\n",
    "def sample_model(set_mod, set_clean, model, num_imgs=2, random=True):\n",
    "    # set the index of the sampled images\n",
    "    index = 0\n",
    "    if random:\n",
    "        index = np.random.randint(0, len(set_mod))\n",
    "\n",
    "    # select the modified/perturbed images\n",
    "    test_images = set_mod[index:index+num_imgs]\n",
    "    # perform a prediction on these images\n",
    "    test_results = model.predict(test_images)\n",
    "    # select trained images or the ground truth\n",
    "    train_images = set_clean[index:index+num_imgs]\n",
    "\n",
    "    sample_predictions = []\n",
    "    for i in range(num_imgs):\n",
    "        sample_predictions.append(test_images[i])\n",
    "        sample_predictions.append(test_results[i])\n",
    "        sample_predictions.append(train_images[i])\n",
    "\n",
    "    plot_images(sample_predictions, ['test','result','train']*num_imgs, n_split=3, colormap='gray')\n",
    "\n",
    "sample_model(test_blur_noise, test, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply model to image of Col. Collins\n",
    "Both the strict input dimension method and the general, size-ambiguous methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IMAGE OF COL. EILEEN COLLINS INTO MEMORY AND APPLY MODIFICATIONS\n",
    "#----------------------------------------------------------------\n",
    "eileen_base = rgb2gray(data.astronaut()).astype('float32')\n",
    "psf = np.ones((5,5))\n",
    "psf /= np.sum(psf)\n",
    "\n",
    "eileen_blur = conv_prod(eileen_base,psf)\n",
    "\n",
    "# If we add a generic distortion in the form of random noise we get:\n",
    "random_mask = 0.1 * np.random.randn(eileen_base.shape[0],eileen_base.shape[1])\n",
    "f_noise = eileen_base + random_mask\n",
    "eileen_noise = normalize_image(f_noise)\n",
    "h_noise = eileen_blur + random_mask\n",
    "eileen_blur_noise = normalize_image(h_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY PREDICTION TO FULL IMAGE (28pixel method)\n",
    "#----------------------------------------------------------------\n",
    "# resize from a 512x512 image to 28x28 (add dummy dimensions to fit the input shape)\n",
    "eileen_blur_noise_reshape = cv2.resize(eileen_blur_noise, (28, 28))\n",
    "eileen_blur_noise_reshape = np.expand_dims(eileen_blur_noise_reshape, axis=2)\n",
    "eileen_blur_noise_reshape = np.expand_dims(eileen_blur_noise_reshape, axis=0)\n",
    "\n",
    "# finally we can apply our model to the blurred, noisy and significantly resized image\n",
    "eileen_result = model.predict(eileen_blur_noise_reshape)\n",
    "\n",
    "plot_images([\n",
    "        eileen_blur_noise, eileen_blur_noise_reshape[0], eileen_result[0]\n",
    "    ],[\n",
    "        f'blurred+noise', f'reshaped', f'prediction'\n",
    "    ], colormap='gray', n_split=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUTTING UP EILEEN\n",
    "#----------------------------------------------------------------\n",
    "eileen_chunks = into_chunks(eileen_blur_noise)\n",
    "# optionally plot\n",
    "#plot_images(eileen_chunks[0], n_split=eileen_chunks[1], colormap='gray', make_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING MODEL TO CHUNKS\n",
    "#----------------------------------------------------------------\n",
    "eileen_chunks_reshaped = []\n",
    "for i in range(eileen_chunks[0].shape[0]):\n",
    "    reshape_0 = cv2.resize(eileen_chunks[0][i], (28, 28))\n",
    "    reshape_1 = np.expand_dims(eileen_chunks[0][i], axis=2)\n",
    "    eileen_chunks_reshaped.append(reshape_1)\n",
    "eileen_chunks_reshaped = np.asarray(eileen_chunks_reshaped)\n",
    "\n",
    "eileen_chunks_prediction = model.predict(eileen_chunks_reshaped, batch_size=32)\n",
    "plot_images(eileen_chunks_prediction, n_split=eileen_chunks[1], colormap='gray', make_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY PREDICTION TO FULL IMAGE (full method)\n",
    "# we can apply our model to the blurred noisy image\n",
    "eileen_result = model.predict(batchify(eileen_blur_noise))\n",
    "# define the iRL deconvolution of the prediction result\n",
    "eileen_c_blur_noise_result = get_kernel(eileen_result[0,:,:,0],psf,mode='crop')\n",
    "eileen_cr_blur_noise_result = np.pad(eileen_c_blur_noise_result, int(np.ceil(eileen_base.shape[0]*0.1)), mode='reflect')\n",
    "# compute both the clipped and non-clipped iRL deconvolution results\n",
    "ml_irl_cr_blur_noise_result_3_15 = get_kernel(RL_iter(eileen_cr_blur_noise_result,psf,eileen_base,3,15,clip_f=True)[0],eileen_base)\n",
    "# print similarity mertics\n",
    "print_quant('ML denoised image',eileen_result[0,:,:,0],eileen_base)\n",
    "print_quant('iRL with i=3,k=15, crop+refl. of ML denoised image',ml_irl_cr_blur_noise_result_3_15,eileen_base)\n",
    "# plot\n",
    "plot_images([\n",
    "        eileen_blur_noise, eileen_result[0], ml_irl_cr_blur_noise_result_3_15\n",
    "    ],[\n",
    "        f'blur+noise', f'prediction', f'deconv'\n",
    "    ], colormap='gray', n_split=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model and run predictions (noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING SELECTED MODEL (noise)\n",
    "#----------------------------------------------------------------\n",
    "if latest_model != 'noise':\n",
    "    train_model(name='noise', log=False, train_in=train_noise, train_target=train, test_in=test_noise, test_target=test)\n",
    "    latest_model = 'noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY PREDICTION TO FULL IMAGE (NOISE ONLY)\n",
    "#----------------------------------------------------------------\n",
    "# we can apply our model to the noisy image\n",
    "eileen_result = model.predict(batchify(eileen_noise))\n",
    "# define the iRL deconvolution of the prediction result\n",
    "eileen_c_noise_result = get_kernel(eileen_result[0,:,:,0],psf,mode='crop')\n",
    "eileen_cr_noise_result = np.pad(eileen_c_noise_result, int(np.ceil(eileen_base.shape[0]*0.1)), mode='reflect')\n",
    "# compute both the clipped and non-clipped iRL deconvolution results\n",
    "ml_irl_cr_noise_result_3_15 = get_kernel(RL_iter(eileen_cr_noise_result,psf,eileen_base,3,15,clip_f=True)[0],eileen_base)\n",
    "# print similarity mertics\n",
    "print_quant('ML denoised image',eileen_result[0,:,:,0],eileen_base)\n",
    "print_quant('iRL with i=3,k=15, crop+refl. of ML denoised image',ml_irl_cr_noise_result_3_15,eileen_base)\n",
    "# plot\n",
    "plot_images([\n",
    "        eileen_noise, eileen_result[0], ml_irl_cr_noise_result_3_15\n",
    "    ],[\n",
    "        f'noise', f'prediction', f'deconv'\n",
    "    ], colormap='gray', n_split=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model and run predictions (blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING SELECTED MODEL (blur)\n",
    "#----------------------------------------------------------------\n",
    "if latest_model != 'blur':\n",
    "    train_model(name='blur', log=False, train_in=train_blur, train_target=train, test_in=test_blur, test_target=test)\n",
    "    latest_model = 'blur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY PREDICTION TO FULL IMAGE (BLUR ONLY)\n",
    "#----------------------------------------------------------------\n",
    "# we can apply our model to the blurred image\n",
    "eileen_result = model.predict(batchify(eileen_blur))\n",
    "# define the iRL deconvolution of the prediction result\n",
    "eileen_c_blur_result = get_kernel(eileen_result[0,:,:,0],psf,mode='crop')\n",
    "eileen_cr_blur_result = np.pad(eileen_c_blur_result, int(np.ceil(eileen_base.shape[0]*0.1)), mode='reflect')\n",
    "# compute both the clipped and non-clipped iRL deconvolution results\n",
    "ml_irl_cr_blur_result_3_15 = get_kernel(RL_iter(eileen_cr_blur_result,psf,eileen_base,3,15,clip_f=True)[0],eileen_base)\n",
    "# print similarity mertics\n",
    "print_quant('ML denoised image',eileen_result[0,:,:,0],eileen_base)\n",
    "print_quant('iRL with i=3,k=15, crop+refl. of ML denoised image',ml_irl_cr_blur_result_3_15,eileen_base)\n",
    "# plot\n",
    "plot_images([\n",
    "        eileen_blur, eileen_result[0], ml_irl_cr_blur_result_3_15\n",
    "    ],[\n",
    "        f'blur', f'prediction', f'deconv'\n",
    "    ], colormap='gray', n_split=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b9f32fa77109cbb6f3793916f643b56b2c23a0b444cf8c07ca9ecd45613c7d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
